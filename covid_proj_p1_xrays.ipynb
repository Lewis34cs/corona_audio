{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "covid_proj_p1_xrays",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "175EDoT7ACYu5JbIgEoFx9DboHzGiWdq2",
      "authorship_tag": "ABX9TyNrOejCETO2vdxvTPgIGOwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lewis34cs/corona_audio/blob/main/covid_proj_p1_xrays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7oaeJQ5N7R_"
      },
      "source": [
        "# Capstone Project Part 1: Chest X-Ray Multi Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv8QBD0NXStb"
      },
      "source": [
        ">Author: Chris Lewis\r\n",
        "\r\n",
        ">Contact info: lewis34cs@gmail.com\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DJlDc99-sC5"
      },
      "source": [
        ">- Link to [Project repo](https://github.com/Lewis34cs/corona_audio)\r\n",
        "  - Part 1: [`covid_proj_p1_xrays.ipynb`](https://colab.research.google.com/drive/175EDoT7ACYu5JbIgEoFx9DboHzGiWdq2#scrollTo=2DJlDc99-sC5)(This Notebook)\r\n",
        "  - Part 2: [`covid_proj_p2_audio.ipynb`](https://colab.research.google.com/drive/1s4z-GJnNkE2NCx5MVccXxXzvBIc__tTX#scrollTo=WjwyDrj8S80W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdX9rBGVN-8H"
      },
      "source": [
        "#### COVID-19 Exploration in statistics and diagnosis via imaging and audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO6J-MeaONhy"
      },
      "source": [
        ">Since the beginning of 2020, COVID-19 has run rampant throughout the entirety of the globe, resulting in over 100 million cases and 2 million deaths. While vaccines are beginning to be pushed out as our new line of defense, my project's goal is to try and identify other ways to detect COVID-19 in a patient to help slow the spread. In this project, I focus on two different ways to identify COVID-19: multi classification via chest x-rays and binary classification through coughing audio. Both parts will be using Sequential Convolutional Neural Networks. This notebook will focus on COVID-19 diagnosis using chest x-ray images obtained from Kaggle's API.\r\n",
        "\r\n",
        ">To run the Google Colab code, you must allow Google Colab to access an account. I will provide links for the data below. Please be sure to save zipped folders for these audio datasets into your google drive so you can access them on Google Colab. The chest x-ray database was obtained via Kaggle's API.\r\n",
        "\r\n",
        ">**Chest X-ray Database**: https://www.kaggle.com/tawsifurrahman/covid19-radiography-database\r\n",
        "---\r\n",
        ">**Link for functions and zipped audio datasets**: https://drive.google.com/drive/folders/1IQ758RksB0ayUhOohfGh8s25MT4taMsI?usp=sharing\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTqHVSBE6cnS"
      },
      "source": [
        "Click on the \"add to drive\" button once you've clicked on the Google Drive link above to add the datasets and functions to your Google Drive\r\n",
        "<img src='https://drive.google.com/uc?id=1YYdx9_xhHRsYLqV2X7yUJauG9zKX_LoX'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3iYW67M_dB6"
      },
      "source": [
        "#### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWMevt1_LJI8"
      },
      "source": [
        ">We must run this in order for colab to be able to access our google drive. We can have google colab mount our google drive by either running the cell below or clicking on the folder icon on the left navigation bar and then clicking on the \"Mount Drive\" button at the top of the navigation pane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqj7Xn3qHOS7",
        "outputId": "41256326-3814-466f-bb48-06c982e7d1fa"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho2msxfSS86V"
      },
      "source": [
        "# We need to import these libraries in order to access the shared folder\r\n",
        "import os, sys"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR1V5RXPKLhh",
        "outputId": "ef4d7210-caac-41a6-81c5-1ce8b601a880"
      },
      "source": [
        "# We must install these libraries for our imported functions folder to work\r\n",
        "!pip install split-folders\r\n",
        "!pip install split-folders tqdm\r\n",
        "!pip install pydub\r\n",
        "!pip install soundfile\r\n",
        "!pip install ffmpeg-python\r\n",
        "!pip install zip_files\r\n",
        "!pip install lime\r\n",
        "!pip install tensorflow_addons"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n",
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/74/e6258e98c8fd43ca8e74a411a3a163ac2f12fc2374d2a609c23442ec90a5/pydub-0.25.0-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.0\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Collecting zip_files\n",
            "  Downloading https://files.pythonhosted.org/packages/f3/99/fc7f65a052d30e62b946924ed6334f8f1ebf7c826646b9e17ab1dff2901c/zip_files-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from zip_files) (7.1.2)\n",
            "Installing collected packages: zip-files\n",
            "Successfully installed zip-files-0.3.0\n",
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-cp37-none-any.whl size=283846 sha256=a933ec2d81f29c63a790741e4cf78df05aafae1b8e8e0bdbc15cc5a2eb65ab4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc0q1vK2Lfw5"
      },
      "source": [
        ">Set the 'FOLDER' variable as the filepath where the folder containing functions.py and audio datasets are located in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Juk01kTzTJ6e",
        "outputId": "f9676bba-24fb-46aa-a174-d161dbbfbda9"
      },
      "source": [
        "FOLDER = '/content/drive/MyDrive/chris_lewis_capstone/'\r\n",
        "os.listdir(FOLDER)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['public_dataset.zip',\n",
              " 'virufy_data-main.zip',\n",
              " '__pycache__',\n",
              " 'public_dataset_v2.zip',\n",
              " 'functions.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Ewd0U2Lnyx"
      },
      "source": [
        ">Defining a path variable (our FOLDER) to add a specific path for the interpreter to search for and then load the filepath into the extensions in order to import all the functions from the functions.py file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjwyDrj8S80W"
      },
      "source": [
        "sys.path.append(FOLDER)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr-WjJ2TiAz"
      },
      "source": [
        "%load_ext autoreload\r\n",
        "%autoreload 2\r\n",
        "from functions import *"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OIQJyJIdTKz"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22zeCeIL751"
      },
      "source": [
        ">These are the libraries we used in the project. Note that it is not necessary to run these cells because the functions.py file has already imported the libraries for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-euk8zDHjRj"
      },
      "source": [
        "#Setting seeds first in order to achieve better consistency in scoring\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "np.random.seed(321)\r\n",
        "tf.random.set_seed(321)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJrDwLTlFnA9"
      },
      "source": [
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "%matplotlib inline\r\n",
        "import os, sys, glob, shutil"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpotBbfoHnPZ"
      },
      "source": [
        "from sklearn import metrics\r\n",
        "from sklearn.dummy import DummyClassifier\r\n",
        "from sklearn.utils import class_weight\r\n",
        "from skimage.segmentation import mark_boundaries\r\n",
        "import time\r\n",
        "from IPython.display import Image, display\r\n",
        "from PIL import Image as Im\r\n",
        "import cv2\r\n",
        "import shutil\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "# tensorflow libraries\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\r\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\r\n",
        "from tensorflow.keras import models, layers, optimizers, regularizers\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO52T4NkKN3c"
      },
      "source": [
        "import splitfolders\r\n",
        "import lime\r\n",
        "from lime import lime_image\r\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYe0h27VDaKI"
      },
      "source": [
        "# setting plot background style\r\n",
        "plt.style.use('dark_background')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNTjfsKerhc"
      },
      "source": [
        "# PART 1: Classifying COVID-19 Chest X-ray Images via Sequential Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BpNGluXDHt9N",
        "outputId": "9fb938a0-dfa1-4b3d-c677-7f1210c800a1"
      },
      "source": [
        "# Viewing where we are in the directory\r\n",
        "os.path.abspath(os.curdir)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwWBJSFweJ9h"
      },
      "source": [
        "## Obtaining COVID CXR dataset via Kaggle API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLalmBaSB0yu"
      },
      "source": [
        ">In order to obtain the chest x-ray database from Kaggle, we make a call to Kaggle's API using an API key. This database still updates regularly, which means that more images are placed into each class, giving us more data to train off of. Instead of downloading the file manually to our google drive, we prefer this route in order to avoid having to constantly check for database updates. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EytsKXluWNku"
      },
      "source": [
        ">The dataset for this part can be found on Kaggle:\r\n",
        "-  Chest X-ray database: \r\n",
        "  - https://www.kaggle.com/tawsifurrahman/covid19-radiography-database. \r\n",
        "- For instructions on how to get an API key: https://www.kaggle.com/docs/api\r\n",
        "\r\n",
        ">Once you have the API key, please upload it in the cell below.\r\n",
        "The below cells are retrieved from Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciGnyx3LwAc"
      },
      "source": [
        "**UPDATE** On March 6th 2021, the Kaggle dataset used in this project was updated - this included more images and a new class: lung_opacity. As of right now, this new class has not yet been included in the majority of the below code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Pz-ogYMz2pS2",
        "outputId": "968f44e4-1dc0-408b-f94c-9531376d8465"
      },
      "source": [
        "# Hiding my key by having a variable store the info\r\n",
        "API_key = files.upload()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6f5ff4c7-4767-4055-b4b6-15d4de5dcb8c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6f5ff4c7-4767-4055-b4b6-15d4de5dcb8c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeqmKabb3Xoq"
      },
      "source": [
        "# Creating a hidden folder then copying my API key and putting it in that file\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXHtyNSJ3ffL"
      },
      "source": [
        "# Permissions of 600 mean that I (the owner) have full read and write access to the \r\n",
        "# file, while no other user can access or edit the file\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dThfIR33qJn",
        "outputId": "9deb8726-1604-4646-cb52-4512cffd2909"
      },
      "source": [
        "# Downloading the zipped dataset via Kaggle's API\r\n",
        "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading covid19-radiography-database.zip to /content\n",
            " 97% 721M/745M [00:14<00:00, 55.2MB/s]\n",
            "100% 745M/745M [00:14<00:00, 53.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLiPTniJ8rqQ"
      },
      "source": [
        "#### Unzipping Folder Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwjQBaONNTDF"
      },
      "source": [
        ">Unzipping the file and extracting to the root directory. If you chose to directly download the database from the kaggle page instead of making a call to the API, replace the 'f_name' variable with the location of the zipped file in your Google Drive. Do not unzip the zipped folder before loading into Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k69IMKaj4TK-",
        "outputId": "b386679d-6965-47ca-c2b4-eea6f46f4038"
      },
      "source": [
        "f_name = \"covid19-radiography-database.zip\"\r\n",
        "\r\n",
        "with ZipFile(f_name, 'r') as zipf:\r\n",
        "      zipf.extractall()\r\n",
        "      print('Done')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxbLD-47JVH"
      },
      "source": [
        "# removing unnecessary files\r\n",
        "!rm /content/covid19-radiography-database.zip\r\n",
        "!rm /content/kaggle.json"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiBfPjin5BpZ",
        "outputId": "05287527-4135-4927-8fe0-3d863e3cee00"
      },
      "source": [
        "# Setting directory location to root directory\r\n",
        "%cd ~\r\n",
        "%cd .."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sd-z-7l1wq4"
      },
      "source": [
        "# setting a variable called cxr_base_folder that contains the directory of where \r\n",
        "# our chest x-ray images are\r\n",
        "cxr_base_folder = ensure_filepath('/content/COVID-19_Radiography_Dataset')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "cAtQb-qE0-2n",
        "outputId": "222838bf-ce08-4ac2-ce81-2944fa3372ac"
      },
      "source": [
        "# Viewing the contents within our cxr_base_folder\r\n",
        "folder_contents(cxr_base_folder)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contents of /content/COVID-19_Radiography_Dataset/:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['COVID',\n",
              " 'COVID.metadata.xlsx',\n",
              " 'Lung_Opacity',\n",
              " 'Lung_Opacity.metadata.xlsx',\n",
              " 'Normal',\n",
              " 'Normal.metadata.xlsx',\n",
              " 'README.md.txt',\n",
              " 'Viral Pneumonia',\n",
              " 'Viral Pneumonia.metadata.xlsx']"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjN9JiVe8AJ"
      },
      "source": [
        "#### Identifying the number of images in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaXdBPyx4qbg",
        "outputId": "a4be5da2-1ca9-49a3-a87b-28b2c3411e44"
      },
      "source": [
        "total_image_number = num_items(cxr_base_folder, \r\n",
        "                               targets=['COVID', 'Normal', 'Viral Pneumonia', 'Lung_Opacity'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/COVID-19_Radiography_Dataset/\n",
            "Number of items in COVID: 3616\n",
            "Number of items in Normal: 10192\n",
            "Number of items in Viral Pneumonia: 1345\n",
            "Number of items in Lung_Opacity: 6012\n",
            "Total number of items: 21165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jHjNyf9MSD"
      },
      "source": [
        ">From here, we view the extracted contents and count the number of images per class along with the total image count. We see that we are working with 3800+ images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TsOQCWgfCDk"
      },
      "source": [
        "### Splitting our images into train, test, and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5hpVo3sOLQP"
      },
      "source": [
        ">Here, we are making a folder to store our train, test, and validation folders. If you would like to store the folders in another filepath, please specify where you would like to store your train, test, and validation folders in the 'cxr_folders' variable. Note that this folder is temporary and will be deleted later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ZtlCYyb3_E"
      },
      "source": [
        "cxr_folders = ensure_filepath('/content/cxr_folders')\r\n",
        "\r\n",
        "os.makedirs(cxr_folders, exist_ok=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fPEoz5s9TPc"
      },
      "source": [
        "#### Temporary Folder Creation & Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze3fL6EtOfRG"
      },
      "source": [
        ">The splitfolders library is extremely effective and very easy to use. It allows us to feed in a directory address (base_folder variable) and give it a place to put the new train, test, and validation folders (output variable). Note that the output address must already be existing. We also set a seed in order to help with model score reproducibility when training our models. The ratio parameter required floats that determines the percentage of data going to each folder.\r\n",
        "- For more info about the splitfolders library, click here: https://github.com/jfilter/split-folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXJXVHTv8Tgt",
        "outputId": "b55285e0-970f-448f-839e-3f7a749de7bd"
      },
      "source": [
        "splitfolders.ratio(input=cxr_base_folder, \r\n",
        "                    output=cxr_folders, \r\n",
        "                    seed=42, ratio=(0.7, 0.1, 0.2)) # ratio order: train, val, test"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 21165 files [00:03, 6283.45 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SUIl7VXB96M"
      },
      "source": [
        "## Recommendation:\r\n",
        "\r\n",
        ">If you prefer working with folder structures when dealing with training and testing data, I would highly recommend using the splitfolders library. Through the use of this library, I was able to create train, test, and validation folders, create image data generators, and have those generators access the data by flowing the data from my directories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ivWqCPpszaa",
        "outputId": "2421005f-c4f3-4117-f742-cb9b4e721d34"
      },
      "source": [
        "folder_check(new_dir=cxr_folders, orig_dir=cxr_base_folder, check=1, )"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cxr_folders/train/\n",
            "Number of items in COVID: 2531\n",
            "Number of items in Lung_Opacity: 4208\n",
            "Number of items in Viral Pneumonia: 941\n",
            "Number of items in Normal: 7134\n",
            "Total number of items: 14814\n",
            "\n",
            "/content/cxr_folders/test/\n",
            "Number of items in COVID: 724\n",
            "Number of items in Lung_Opacity: 1203\n",
            "Number of items in Viral Pneumonia: 270\n",
            "Number of items in Normal: 2039\n",
            "Total number of items: 4236\n",
            "\n",
            "/content/cxr_folders/val/\n",
            "Number of items in COVID: 361\n",
            "Number of items in Lung_Opacity: 601\n",
            "Number of items in Viral Pneumonia: 134\n",
            "Number of items in Normal: 1019\n",
            "Total number of items: 2115\n",
            "\n",
            "/content/COVID-19_Radiography_Dataset/\n",
            "Number of items in COVID: 3616\n",
            "Number of items in Lung_Opacity: 6012\n",
            "Number of items in Viral Pneumonia: 1345\n",
            "Number of items in Normal: 10192\n",
            "Total number of items: 21165\n",
            "\n",
            "Are the image numbers equal?\n",
            "True\n",
            "\n",
            "Total image count: 21165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez6f3Iw_AT_2"
      },
      "source": [
        "# Creating variables to hold our addresses for our train, test, and val folders\r\n",
        "train_folder, test_folder, val_folder = create_ttv(cxr_folders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwrNlYjAWuyT"
      },
      "source": [
        "train_folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFh0icmmQDEZ"
      },
      "source": [
        ">Since we are using a DummyClassifier as our baseline model, it cannot work with ImageDataGenerators. So instead, we will create X and y train, test, and validation variables by feeding in a batch size equal to the amount of data in each train, test, and validation folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHcxc1V5BP1Z"
      },
      "source": [
        "TRAIN_BATCH_SIZE, TEST_BATCH_SIZE, VAL_BATCH_SIZE = batch_sizes([train_folder, \r\n",
        "                                                                 test_folder, \r\n",
        "                                                                 val_folder])\r\n",
        "print(f\"Train batch size: {TRAIN_BATCH_SIZE}\\nTest batch size: {TEST_BATCH_SIZE}\"\r\n",
        "      f\"\\nValidation batch size: {VAL_BATCH_SIZE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSVWxYe8ezSR"
      },
      "source": [
        "#### Setting up ImageDataGenerators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7_jF2BZQmaH"
      },
      "source": [
        ">Setting up our variables for the dummy classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2MV1_PofrEG"
      },
      "source": [
        "# Defining our image size\r\n",
        "IMG_SIZE = (128, 128)\r\n",
        "\r\n",
        "# Creating ImageDataGenerators for train and test/val\r\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "test_val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVTrA1ongY9X"
      },
      "source": [
        "dummy_train_generator = train_datagen.flow_from_directory(train_folder, \r\n",
        "                                                   target_size=IMG_SIZE,\r\n",
        "                                                   batch_size=TRAIN_BATCH_SIZE,\r\n",
        "                                                   class_mode='categorical')\r\n",
        "\r\n",
        "dummy_test_generator = test_val_datagen.flow_from_directory(test_folder, \r\n",
        "                                                     target_size=IMG_SIZE,\r\n",
        "                                                     batch_size=TEST_BATCH_SIZE,\r\n",
        "                                                     class_mode='categorical')\r\n",
        "\r\n",
        "dummy_val_generator = test_val_datagen.flow_from_directory(val_folder, \r\n",
        "                                                    target_size=IMG_SIZE,\r\n",
        "                                                    batch_size=VAL_BATCH_SIZE,\r\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-l8Obdqhsiv"
      },
      "source": [
        "# Viewing the classes within our iterator\r\n",
        "dummy_train_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTH7GcjnQtQL"
      },
      "source": [
        ">Here is where we create our X and y variables for each train, test, and validation set. Notice that by calling next() on each iterator, we are feeding X and y variables data and labels equivalent to the batch sizes of the respective entire folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZAg2IW0hv2X"
      },
      "source": [
        "start = time.time()\r\n",
        "# Creating X and y values for dummy train, test, and validation sets\r\n",
        "X_train, y_train = next(dummy_train_generator)\r\n",
        "X_test, y_test = next(dummy_test_generator)\r\n",
        "X_val, y_val = next(dummy_val_generator)\r\n",
        "\r\n",
        "end = time.time()\r\n",
        "time_count(start, end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svd7sZlpNdkY"
      },
      "source": [
        "random_image(X_train, np.argmax(y_train, axis=1), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bv6zgldhCoO"
      },
      "source": [
        "#### Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ4mcWMoREWt"
      },
      "source": [
        ">We need to one-hot encode our classes from our generated labels in order for the confusion matrix to be able to understand the we are working with more than two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehWzrZxCiASj"
      },
      "source": [
        "# Creating and fitting our dummy classifier\r\n",
        "dummy = DummyClassifier(strategy='stratified')\r\n",
        "dummy.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Creating our y_pred variable\r\n",
        "y_pred = dummy.predict(X_test)\r\n",
        "\r\n",
        "y_pred = np.argmax(y_pred, axis=1)\r\n",
        "y_test_ohe = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eHnzUGInXWA"
      },
      "source": [
        "#Printing classification report and plotting confusion matrix\r\n",
        "print(metrics.classification_report(y_test_ohe, y_pred));\r\n",
        "\r\n",
        "plt.figure(figsize=(7, 6))\r\n",
        "cm = metrics.confusion_matrix(y_test_ohe, y_pred, labels = [0, 1, 2], \r\n",
        "                              normalize='true')\r\n",
        "sns.heatmap(cm, cmap=\"Greens\", annot=True, square=True)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjGVMJwenHVe"
      },
      "source": [
        ">We built a baseline dummy classifier model that has an accuracy of 34%, and it tends to guess for the class == 0, which is our Covid class. Since our dataset is relatively balanced in terms of classes, we expect to see the accuracy be around 33%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcU-41f9BLIE"
      },
      "source": [
        "## First CNN Model for CXRs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xlVOAhRAnQL"
      },
      "source": [
        ">Now we will create new generators that we will feed into a CNN model. Notice for the test generator we must set the parameter shuffle=False. This is because when we want to evaluate the model, we want be able to see the model's actual score against the test generator. If we were to set shuffle=True for the test generator, the labels in the test generator would not line up with the images when trying to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x88yIEVOuFEI"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_folder, \r\n",
        "                                                   target_size=IMG_SIZE,\r\n",
        "                                                   batch_size=32,\r\n",
        "                                                   class_mode='categorical')\r\n",
        "\r\n",
        "test_generator = test_val_datagen.flow_from_directory(test_folder, \r\n",
        "                                                     target_size=IMG_SIZE,\r\n",
        "                                                     batch_size=32,\r\n",
        "                                                     class_mode='categorical', \r\n",
        "                                                     shuffle=False)\r\n",
        "\r\n",
        "val_generator = test_val_datagen.flow_from_directory(val_folder, \r\n",
        "                                                    target_size=IMG_SIZE,\r\n",
        "                                                    batch_size=32,\r\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdMavfRbHcRG"
      },
      "source": [
        "# Defining input shape to feed into our CNN\r\n",
        "INPUT_SHAPE = train_generator.image_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Xca2Jko8CN"
      },
      "source": [
        "#### Viewing our Class Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1_pUY31RXws"
      },
      "source": [
        ">We will now create a dictionary containing the weights for each class by using our custom make_class_weights() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P5I5uYJUpAu"
      },
      "source": [
        "class_weights_dict = make_class_weights(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ__nNrnI8-y"
      },
      "source": [
        "### Creating CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhc68bQjGAhm"
      },
      "source": [
        "def create_basic_cnn(shape='',\r\n",
        "                     metrics=['acc', tf.keras.metrics.Precision(name='precision'),\r\n",
        "                              tf.keras.metrics.Recall(name='recall'), \r\n",
        "                              tf.keras.metrics.AUC(name='auc')]):\r\n",
        "  \"\"\"\r\n",
        "  Definition:\r\n",
        "  Creates a basic cnn model consisting of three layers and an output layer.\r\n",
        "\r\n",
        "  Args:\r\n",
        "  shape: this function requires an input shape in order for the model to be created.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "  Returns a compiled model.\r\n",
        "  \"\"\"\r\n",
        "  # We are requiring an input shape since we print the model.summary() at the \r\n",
        "  # end of the function\r\n",
        "  if len(shape) < 1:\r\n",
        "    print('Please define input shape!')\r\n",
        "  else:\r\n",
        "    # Input layer\r\n",
        "    model = Sequential()\r\n",
        "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=shape))\r\n",
        "    model.add(MaxPooling2D((2, 2)))\r\n",
        "\r\n",
        "    # Second layer\r\n",
        "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\r\n",
        "    model.add(MaxPooling2D((2, 2)))\r\n",
        "      \r\n",
        "    # Third layer\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(layers.Dense(128, activation='relu'))\r\n",
        "      \r\n",
        "    # Output layer\r\n",
        "    model.add(layers.Dense(3, activation='softmax'))\r\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=metrics)\r\n",
        "    print(model.summary())\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1SSJiQqIUxU"
      },
      "source": [
        "basic_model = create_basic_cnn(shape=INPUT_SHAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GGLJPDe6M5_"
      },
      "source": [
        "### Fitting model, Plotting scores, Reporting evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPzu7F75Rj9d"
      },
      "source": [
        ">The fit_plot_report_gen() function takes in a CNN model and fits it on a training generator. After each epoch the validation generator is evaluated, which allows us to see if the model is learning or overfitting. Once the model finishes training, we evaluate the test generator and create a classification report, confusion matrix, and a plot for each metric that was scored over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw2hdWCAdhg9"
      },
      "source": [
        "hist1 = fit_plot_report_gen(basic_model, train_generator, test_generator, val_generator, \r\n",
        "                class_weights=class_weights_dict, epochs=5, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n9LOLcdDATt"
      },
      "source": [
        ">While the above model seems to be performing extremely well with an accuracy of 94.6%, we would like to improve the recall rate (94.6%) for our COVID class. Since COVID-19 is able to spread so easily, the less people that are diagnosed as false negatives, the better. 6% may seem like a small number of false negatives, but if we were to diagnose 1,000,000 people who were COVID positive, we would have around 60,000 COVID positive cases that were wrongly identified as being another class.\r\n",
        "\r\n",
        ">Based on prior experience when working with pneumonia chest x-rays in my previous project, it is extremely important for us to feed in chest x-ray images of high quality in order to score extremely well. For this next portion, we will be using CLAHE as a preprocessing technique to enhance the images before they are fed into the model. In doing so, hopefully we will see an increase in recall rate for the COVID class and an overall increase in accuracy. \r\n",
        "\r\n",
        ">- Link to [Pneumonia Project repo](https://github.com/Lewis34cs/cloudy_lungs)\r\n",
        "  - Part 1: [`pneumonia_proj.ipynb`](https://colab.research.google.com/drive/1pMKRsWUJ5pnAGUp_W01nB9fitP57F0Sc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0E1fNRXEiq9"
      },
      "source": [
        "# removing unpreprocessed CXR train, test, and val folder directory\r\n",
        "try:\r\n",
        "  shutil.rmtree(cxr_folders)\r\n",
        "except:\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCHNNzvLUi0"
      },
      "source": [
        "## CLAHE Preprocessing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynVOM8qnBwZy"
      },
      "source": [
        ">Contrast Limited Adaptive Histogram Equalization (CLAHE) is used to equalize pixel intensity in images. It is very similar to Adaptive Histogram Equalization (AHE), except it doesn't over-amplify the contrast of the image. This is controlled by the clipLimit parameter. The way CLAHE works on an image is that it focuses on small portions of the image (tileGridSize parameter) and then combines these portions together through bilinear interpolation to help remove any artificial boundaries, which means that it enhances the local contrast of the total image. This essentially helps with the pixel intensity distribution, allowing us to see more \"depth\" in an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foGMXTKUQqW4"
      },
      "source": [
        ">link for info on cv2.createCLAHE(): https://docs.opencv.org/master/d6/dc7/group__imgproc__hist.html#gad689d2607b7b3889453804f414ab1018"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqzwkHBBUGYE"
      },
      "source": [
        ">Here we are splitting up classes so we are able to view an example image of each class before CLAHE and then comparing the before-and-after images side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWdtiheYCn1S"
      },
      "source": [
        "# Defining classes\r\n",
        "covid = ('Covid Class', 'COVID/COVID')\r\n",
        "normal = ('Healthy Class', 'NORMAL/NORMAL')\r\n",
        "pneumo = ('Viral Pneumonia Class', 'Viral Pneumonia/Viral Pneumonia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnyvpGuvSy8R"
      },
      "source": [
        "class_select = input(f'Please select one of the classes to view: '\r\n",
        "                     f'[covid][normal][pneumo] ')\r\n",
        "if class_select == 'covid':\r\n",
        "  selected_class = covid\r\n",
        "elif class_select == 'normal':\r\n",
        "  selected_class = normal\r\n",
        "elif class_select == 'pneumo':\r\n",
        "  selected_class = pneumo\r\n",
        "try:\r\n",
        "  # choosing a random image within the selected class folder\r\n",
        "  i = np.random.choice(range(len(os.listdir(cxr_base_folder+selected_class[1].split('/')[0]))))\r\n",
        "  img = cv2.imread(cxr_base_folder + selected_class[1] + f' ({i}).png', 0)\r\n",
        "  print(f'Successfully loaded in an image from {selected_class[0]}')\r\n",
        "  print(f\"Image size: {img.shape}\")\r\n",
        "except:\r\n",
        "  print('Error: No class with that name exists')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VVazLFLaf7"
      },
      "source": [
        "# Plotting an image to view, along with distribution of pixel intensity\r\n",
        "\r\n",
        "plt.title(selected_class[0])\r\n",
        "plt.imshow(img, cmap='gray')\r\n",
        "hist,bins = np.histogram(img.flatten(),256,[0,256])\r\n",
        "\r\n",
        "cdf = hist.cumsum()\r\n",
        "cdf_normalized = cdf * hist.max()/ cdf.max()\r\n",
        "\r\n",
        "plt.figure(figsize=(6, 6))\r\n",
        "plt.title('Distribution of Pixel Intensity')\r\n",
        "plt.plot(cdf_normalized, color = 'b')\r\n",
        "plt.hist(img.flatten(),256,[0,256], color = 'r')\r\n",
        "plt.xlim([0,256])\r\n",
        "plt.legend(('cdf','histogram'), loc = 'upper left')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Bd5rlmRkpn"
      },
      "source": [
        ">As we can see here, the distribution of pixel intensity in our unpreprocessed image ranges from 0 to around 220. The way CLAHE works on an image is that it focuses on small portions of the image (tileGridSize parameter) and then combines these portions together through **bilinear interpolation** = a resampling method that uses the distance-­weighted average of the X nearest pixel values to estimate a new pixel value. Bilinear interpolation helps remove any artificial boundaries, which enhances the local contrast of the total image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpBLZE-rMO-"
      },
      "source": [
        "### Creating CLAHE preprocessed images and placing them into new directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYFjyQnVE2M"
      },
      "source": [
        ">Creating a new folder in the 'cxr_clahe_folder' variable to store our CLAHE preprocessed images. If you would like to store the folders in another filepath, please specify where you would like to store your train, test, and validation folders in the 'cxr_clahe_folder' variable. Note that this folder is temporary and will be deleted later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saZimyENxUbN"
      },
      "source": [
        "# Please define a filepath where you would like the CLAHE preprocessed images to\r\n",
        "# be stored in the 'new_folder' variable. This folder is temporary.\r\n",
        "try:\r\n",
        "  if os.path.exists(cxr_clahe_folder):\r\n",
        "    print('CLAHE folder already exists')\r\n",
        "except:\r\n",
        "  cxr_clahe_folder = ensure_filepath('/content/clahe_radio')\r\n",
        "  clahe_preprocessing(cxr_base_folder, cxr_clahe_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ovLmSZVJxEX"
      },
      "source": [
        "num_items(cxr_clahe_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iyCKFbsCN3s"
      },
      "source": [
        "# Setting original and CLAHE images to compare\r\n",
        "orig_img = cv2.imread(cxr_base_folder + selected_class[1] + f' ({i}).png')\r\n",
        "clahe_img = cv2.imread(cxr_clahe_folder + selected_class[1] + f' ({i}).png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUSVrgdSNyG8"
      },
      "source": [
        "hist,bins = np.histogram(clahe_img.flatten(),256,[0,256])\r\n",
        "\r\n",
        "cdf = hist.cumsum()\r\n",
        "cdf_normalized = cdf * hist.max()/ cdf.max()\r\n",
        "\r\n",
        "plt.figure(figsize=(6, 6))\r\n",
        "plt.title('Distribution of Pixel Intensity Using CLAHE')\r\n",
        "plt.plot(cdf_normalized, color = 'b')\r\n",
        "plt.hist(clahe_img.flatten(),256,[0,256], color = 'r')\r\n",
        "plt.xlim([0,256])\r\n",
        "plt.legend(('cdf','histogram'), loc = 'upper left')\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sM2HDu0Wcy1"
      },
      "source": [
        ">Note that we can see that the intensities of pixels, when compared to the original distribution, have become more spread out. This will give our image a 'deeper' look to it, and give our model more information for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt7c0rwnPNHT"
      },
      "source": [
        "# Comparing original image (left) to CLAHE applied image (right)\r\n",
        "res = np.hstack((orig_img,clahe_img))\r\n",
        "fig = plt.figure(figsize=(10, 5))\r\n",
        "ax = fig.add_subplot(111)\r\n",
        "plt.imshow(res, cmap='gray')\r\n",
        "plt.title('Original image                           Modified CLAHE image')\r\n",
        "ax.axes.get_xaxis().set_visible(False)\r\n",
        "ax.axes.get_yaxis().set_visible(False)\r\n",
        "ax.set_frame_on(False)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REd_naWSR6mw"
      },
      "source": [
        ">We can definitely see more of the infiltrate in the lung areas of the chest x-ray. These infiltrate areas of the lung can determine whether or not a person has Pneumonia. We can definitely see more of the infiltrate in the lung areas of the chest x-ray. These infiltrate areas of the lung can determine whether or not a person has Pneumonia. According to Hosseiny et al., when there is radiographic appearances of multifocal ground glass opacity, linear opacities, and consolidation, these are usually seen in cases of coronavirus type infections, including COVID-19, SARS, and MERS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSikbgbvDgdx"
      },
      "source": [
        "# Removing unpreprocessed images from our directory\r\n",
        "try:\r\n",
        "  shutil.rmtree(cxr_base_folder)\r\n",
        "except:\r\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FySQkEHxW1Zu"
      },
      "source": [
        ">Splitting our preprocess image folder into train, test, and validation subfolders into a new directory. If you would like to store the folders in another filepath, please specify where you would like to store your train, test, and validation folders in the 'clahe_ttv_folder' variable. Note that this folder is temporary and will be deleted later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9i2Lgc9pXka"
      },
      "source": [
        "clahe_ttv_folder = ensure_filepath('/content/clahe_ttv')\r\n",
        "\r\n",
        "splitfolders.ratio(input=cxr_clahe_folder, \r\n",
        "                    output=clahe_ttv_folder, \r\n",
        "                    seed=42, ratio=(0.7, 0.1, 0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqjlgrLdL5Vd"
      },
      "source": [
        "# Removing base clahe images folder from our directory\r\n",
        "try:\r\n",
        "  if os.path.exists(clahe_ttv_folder):\r\n",
        "    shutil.rmtree(cxr_clahe_folder)\r\n",
        "except:\r\n",
        "  print('Please run the cell above first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRq-M28cp4wg"
      },
      "source": [
        "folder_check(clahe_ttv_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4INXA2zn040"
      },
      "source": [
        "## Modeling with CLAHE preprocessed images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPzeqnDGrWAE"
      },
      "source": [
        ">Defining new train, test, and validation folders for our CLAHE preprocessed chest x-ray images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbTg-e-mqKfY"
      },
      "source": [
        "train_folder, test_folder, val_folder = create_ttv(clahe_ttv_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFF9AJbzqmh-"
      },
      "source": [
        "# Still the same size as our previous model\r\n",
        "IMG_SIZE = (128, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NRm4PgsrDEy"
      },
      "source": [
        "train_clahe_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "test_val_clahe_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "cl_train_generator = train_clahe_datagen.flow_from_directory(train_folder, \r\n",
        "                                                   target_size=IMG_SIZE,\r\n",
        "                                                   batch_size=32,\r\n",
        "                                                   class_mode='categorical')\r\n",
        "\r\n",
        "cl_test_generator = test_val_clahe_datagen.flow_from_directory(test_folder, \r\n",
        "                                                     target_size=IMG_SIZE,\r\n",
        "                                                     batch_size=32,\r\n",
        "                                                     class_mode='categorical', \r\n",
        "                                                     shuffle=False)\r\n",
        "\r\n",
        "cl_val_generator = test_val_clahe_datagen.flow_from_directory(val_folder, \r\n",
        "                                                    target_size=IMG_SIZE,\r\n",
        "                                                    batch_size=32,\r\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cboKKu8mO-X"
      },
      "source": [
        "INPUT_SHAPE = cl_train_generator.image_shape\r\n",
        "print(INPUT_SHAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXLfDEj5ml3w"
      },
      "source": [
        "class_weights_dict = make_class_weights(cl_train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2wnwmursiUd"
      },
      "source": [
        "basic_prep_cnn = create_basic_cnn(shape=INPUT_SHAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPGZm4JMcae-"
      },
      "source": [
        ">Now that we've preprocessed our images, put the images into train, test, and validation folders, and created generators to feed into our model, we can now fit the model. Feel free to tweak the parameters of the three_callbacks() function. Make sure to read how the function works first if you choose to do so. Below are some recommendations for tweaking the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Z5zGOcmYCB"
      },
      "source": [
        "\"\"\"\r\n",
        "Recommended parameters to play around with:\r\n",
        " - epochs\r\n",
        " - lr_patience\r\n",
        " - factor\r\n",
        " - patience\r\n",
        " - restore_best_weights\r\n",
        "\r\n",
        "Please read the three_callbacks() function before changing!\r\n",
        "\r\n",
        "Note that: \r\n",
        " - patience should not be lower than lr_patience, otherwise learning rate decay\r\n",
        " will not occur.\r\n",
        " - Both patience and lr_patience should not be as large / larger than the number \r\n",
        " of epochs\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "# Please define a filepath to store all models that will be saved from the \r\n",
        "# ModelCheckpoint callback in the variable 'cxr_model_storage'. \r\n",
        "cxr_model_storage = ensure_filepath('/cxr_models/')\r\n",
        "\r\n",
        "prep_hist = fit_plot_report_gen(basic_prep_cnn, cl_train_generator, \r\n",
        "                                cl_test_generator, cl_val_generator, \r\n",
        "                                epochs=10, batch_size=32,\r\n",
        "                                class_weights=class_weights_dict, \r\n",
        "                                callbacks=three_callbacks(lr_patience=3, factor=0.9, \r\n",
        "                                                          patience=5, \r\n",
        "                                                          restore_best_weights=False,\r\n",
        "                                                          f_path=cxr_model_storage))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEYObbsSp7w5"
      },
      "source": [
        "#### Viewing what the model sees using lime image explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el13v9R2c261"
      },
      "source": [
        ">By using the lime library, we create a function that explains what the model determines to be important factors (both positive and negative) when diagnosing an image. Positive factors are marked with green, and negative factors are marked with red. \r\n",
        "\r\n",
        ">For more information about the lime library: https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Cc0SemvWcx"
      },
      "source": [
        "explain_image(basic_prep_cnn, cl_train_generator, num_samples=2000, num_feats=3, \r\n",
        "              class_label=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYdd72tbuByY"
      },
      "source": [
        "### Saving best CXR models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAOHIpwdQSo"
      },
      "source": [
        ">Now we define a filepath where we would like to save our best models. Please set a filepath for the 'best_models_filepath' variable where you would like to store your best models. Feel free to change the threshold, note that the lower the threshold, the more\r\n",
        "models will be saved to the 'best_models_filepath'. Models that do not surpass the threshold are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2tG2jWj9wy2"
      },
      "source": [
        "best_models_filepath = ensure_filepath('/content/drive/MyDrive/gdrive/models/capstone_models/cxr_models/')\r\n",
        "THRESHOLD = 0.96\r\n",
        "\r\n",
        "move_best_models(source=cxr_model_storage, threshold=THRESHOLD,\r\n",
        "                 new_dir=best_models_filepath, \r\n",
        "                 test_gen=cl_test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9KLn_xdjTp"
      },
      "source": [
        ">To load in a saved model, please specify the filepath where you saved the model and put into the 'model_fpath' variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QixDIrKMQV-V"
      },
      "source": [
        "model_fpath = '/content/drive/MyDrive/gdrive/models/capstone_models/cxr_models/model-05-0.101.hdf5'\r\n",
        "\r\n",
        "best_model = load_model(model_fpath)\r\n",
        "class_report_gen(best_model, cl_test_generator, \r\n",
        "                 class_indices=test_generator.class_indices, cmap='Greens')\r\n",
        "best_model.evaluate(cl_test_generator, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYHXNKwuEDoK"
      },
      "source": [
        "## Recommendation: \r\n",
        ">When working with images like x-rays or MRI scans, I highly recommend using CLAHE as a preprocessing technique to create new images that give the model more to learn from. CLAHE also is able to provide enough contrast to the image without overamplifying the intensity of the pixels. It is a great tool if the goal of your project involve detection and/or recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEEbwNmLkHt8"
      },
      "source": [
        "# If you are finished with the above section and would like to continue, please\r\n",
        "# set run = 1 to remove unecessary folders for Part 2\r\n",
        "ans = input(f\"All temporary files from Part 1 will be removed from the working \"\r\n",
        "            f\"directory. Proceed? [y][n] \")\r\n",
        "if ans.lower() == 'y':\r\n",
        "  print(\"Removing files\")\r\n",
        "  try:\r\n",
        "    shutil.rmtree(clahe_ttv_folder)\r\n",
        "    shutil.rmtree(cxr_model_storage)\r\n",
        "    print(\"Process completed\")\r\n",
        "  except:\r\n",
        "    print(\"No folders were removed\")\r\n",
        "else:\r\n",
        "  print(\"No folders were removed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I-xIJKwnqMi"
      },
      "source": [
        "# Part 1 Conclusion\r\n",
        ">The scores of our best model show an improvement in both recall rate (98%) for the COVID class and overall accuracy (97%). By using CLAHE, our model was able to reduce the amount of false negatives from our previous model by 4%. CLAHE is a great preprocessing tool that many hospitals and facilities should use when diagnosing their patients for certain diseases that may affect the targeted x-ray area.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrzBHKFoGvS"
      },
      "source": [
        "#### Caveats\r\n",
        ">While using x-rays to diagnose patients with COVID has proven to be successful with the model we've created, the process of each person getting an x-ray to determine if they have COVID can be costly. Furthermore, we wouldn't want someone with COVID to come into a facility and expose healthy people to the virus. In Part 2, we are going to explore the possiblities of using cough audio from healthy and COVID infected individuals and see if we can create a model that can accurately diagnose those with COVID. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDwRQCOCNZH8"
      },
      "source": [
        "## Part 2: Mel-Spectrogram Binary Classification\r\n",
        "\r\n",
        "Link to Part 2: [`covid_proj_p2_audio.ipynb`](https://colab.research.google.com/drive/1s4z-GJnNkE2NCx5MVccXxXzvBIc__tTX#scrollTo=WjwyDrj8S80W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxVjydxZq0R"
      },
      "source": [
        "## References\r\n",
        "\r\n",
        "1. Virginia Department of Health. “COVID-19 Testing.” Vdh.Virginia.Gov, Virginia Department of Health, 5 Feb. 2021, www.vdh.virginia.gov/coronavirus/covid-19-testing/#:~:text=There%20are%20two%20main%20types,)%20and%20antibody%20tests.\r\n",
        "2. Shmerling, Robert H. “Which Test Is Best for COVID-19?” Harvard Health Blog, 5 Jan. 2021, www.health.harvard.edu/blog/which-test-is-best-for-covid-19-2020081020734#:%7E:text=The%20reported%20rate%20of%20false,infection%20the%20test%20is%20performed.\r\n",
        "3. “Coronavirus Disease 2019 (COVID-19).” Centers for Disease Control and Prevention, 11 Feb. 2020, www.cdc.gov/coronavirus/2019-ncov/more/scientific-brief-sars-cov-2.html#:%7E:text=%E2%80%A2%20Contact%20transmission%20is%20infection,(typically%20hours).\r\n",
        "4. Smyth, Tamara. “The Mel Scale.” University of California, San Diego, Tamara Smyth, 4 June 2019, musicweb.ucsd.edu/~trsmyth/pitch2/Mel_Scale.html.\r\n",
        "5. M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, M. T. Islam, “Can AI help in screening Viral and COVID-19 pneumonia?” IEEE Access, Vol. 8, 2020, pp. 132665 - 132676.\r\n",
        "6. Lara Orlandic, Tomas Teijeiro, & David Atienza. (2020). The COUGHVID crowdsourcing dataset: A corpus for the study of large-scale cough analysis algorithms (Version 1.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.4048312\r\n",
        "7. Chaudhari, Gunvant, et al. \"Virufy: Global Applicability of Crowdsourced and Clinical Datasets for AI Detection of COVID-19 from Cough.\" arXiv preprint arXiv:2011.13320 (2020).\r\n",
        "8. Hosseiny M, Kooraki S, Gholamrezanezhad A, Reddy S, Myers L. Radiology Perspective of Coronavirus Disease 2019 (COVID-19): Lessons From Severe Acute Respiratory Syndrome and Middle East Respiratory Syndrome. AJR Am J Roentgenol2020;214:1078-82. doi:10.2214/AJR.20.22969 pmid:32108495\r\n",
        "9. Zafra, Miguel Fernández. “Understanding Convolutions and Pooling in Neural Networks: A Simple Explanation.” Medium, 25 May 2020, towardsdatascience.com/understanding-convolutions-and-pooling-in-neural-networks-a-simple-explanation-885a2d78f211."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcXa7Qy1MgM"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMdeGDwbhTgU"
      },
      "source": [
        "!pip install git+https://github.com/rcmalli/keras-squeezenet.git\r\n",
        "!pip install keras_applications==1.0.4 --no-deps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2UF8fRp2Dz_"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow.keras\r\n",
        "import \r\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\r\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcBtIp_f1RIs"
      },
      "source": [
        "import numpy as np\r\n",
        "import keras.applications.\r\n",
        "from keras_squeezenet import SqueezeNet\r\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\r\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqMcLtt1sAm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}